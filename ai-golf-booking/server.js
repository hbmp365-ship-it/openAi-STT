import express from 'express'
import multer from 'multer'
import OpenAI from 'openai'
import fs from 'fs'
import path from 'path'
import { fileURLToPath } from 'url'
import cors from 'cors'
import ffmpeg from 'fluent-ffmpeg'

const __filename = fileURLToPath(import.meta.url)
const __dirname = path.dirname(__filename)

const app = express()
const upload = multer({ dest: 'uploads/' })

// CORS ì„¤ì •
app.use(cors())
app.use(express.json())

// OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
})

// ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ìƒì„±
if (!fs.existsSync('uploads')) {
  fs.mkdirSync('uploads')
}

// ì˜¤ë””ì˜¤ íŒŒì¼ ë³€í™˜ í•¨ìˆ˜ (webm -> wav)
const convertAudioToWav = (inputPath, outputPath) => {
  return new Promise((resolve, reject) => {
    ffmpeg(inputPath)
      .toFormat('wav')
      .audioCodec('pcm_s16le')
      .audioChannels(1)
      .audioFrequency(16000)
      .on('end', () => {
        console.log('ì˜¤ë””ì˜¤ ë³€í™˜ ì™„ë£Œ:', outputPath)
        resolve(outputPath)
      })
      .on('error', (err) => {
        console.error('ì˜¤ë””ì˜¤ ë³€í™˜ ì˜¤ë¥˜:', err)
        reject(err)
      })
      .save(outputPath)
  })
}

// STT (Speech-to-Text) ì—”ë“œí¬ì¸íŠ¸
app.post('/api/transcribe', upload.single('audio'), async (req, res) => {
  console.log('=== STT ìš”ì²­ ë°›ìŒ ===')
  console.log('Headers:', req.headers)
  console.log('Body keys:', Object.keys(req.body || {}))
  console.log('File info:', req.file ? {
    fieldname: req.file.fieldname,
    originalname: req.file.originalname,
    filename: req.file.filename,
    size: req.file.size,
    mimetype: req.file.mimetype
  } : 'No file')
  
  try {
    if (!req.file) {
      console.log('ì˜¤ë¥˜: ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŒ')
      return res.status(400).json({ error: 'ì˜¤ë””ì˜¤ íŒŒì¼ì´ í•„ìš”í•©ë‹ˆë‹¤.' })
    }

    console.log('Processing audio file:', req.file.filename)
    console.log('File size:', req.file.size, 'bytes')
    console.log('File type:', req.file.mimetype)

    // OpenAI API í‚¤ í™•ì¸
    const apiKey = process.env.OPENAI_API_KEY
    
    if (!apiKey) {
      console.error('OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.')
      return res.status(500).json({
        success: false,
        error: 'OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
        details: 'í™˜ê²½ ë³€ìˆ˜ OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.'
      })
    }
    
    console.log('OpenAI API í‚¤ í™•ì¸ë¨:', apiKey.substring(0, 20) + '...')

    console.log('OpenAI API í˜¸ì¶œ ì‹œì‘...')
    
    // webm íŒŒì¼ì„ wavë¡œ ë³€í™˜
    const wavPath = req.file.path.replace(/\.[^/.]+$/, '.wav')
    console.log('ì˜¤ë””ì˜¤ ë³€í™˜ ì‹œì‘:', req.file.path, '->', wavPath)
    
    try {
      await convertAudioToWav(req.file.path, wavPath)
      
      // ì›ë³¸ íŒŒì¼ ì‚­ì œ
      fs.unlinkSync(req.file.path)
      console.log('ì›ë³¸ íŒŒì¼ ì‚­ì œë¨:', req.file.path)
      
      // OpenAI Whisper APIë¥¼ ì‚¬ìš©í•˜ì—¬ ìŒì„±ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
      const transcription = await openai.audio.transcriptions.create({
        file: fs.createReadStream(wavPath),
        model: 'whisper-1',
        language: 'ko', // í•œêµ­ì–´ ì„¤ì •
        response_format: 'json'
      })
      
      // ë³€í™˜ëœ íŒŒì¼ ì‚­ì œ
      fs.unlinkSync(wavPath)
      console.log('ë³€í™˜ëœ íŒŒì¼ ì‚­ì œë¨:', wavPath)

      console.log('OpenAI API ì‘ë‹µ ë°›ìŒ')
      console.log('Transcription result:', transcription.text)

      res.json({
        success: true,
        transcription: transcription.text
      })

    } catch (conversionError) {
      console.error('ì˜¤ë””ì˜¤ ë³€í™˜ ì˜¤ë¥˜:', conversionError)
      
      // ì›ë³¸ íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œ
      if (fs.existsSync(req.file.path)) {
        fs.unlinkSync(req.file.path)
        console.log('ì›ë³¸ íŒŒì¼ ì‚­ì œë¨ (ì˜¤ë¥˜)')
      }
      
      // ë³€í™˜ëœ íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œ
      const wavPath = req.file.path.replace(/\.[^/.]+$/, '.wav')
      if (fs.existsSync(wavPath)) {
        fs.unlinkSync(wavPath)
        console.log('ë³€í™˜ëœ íŒŒì¼ ì‚­ì œë¨ (ì˜¤ë¥˜)')
      }
      
      throw conversionError
    }

  } catch (error) {
    console.error('Transcription error:', error)
    console.error('Error stack:', error.stack)
    
    // ì„ì‹œ íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œ
    if (req.file && fs.existsSync(req.file.path)) {
      fs.unlinkSync(req.file.path)
      console.log('ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì¸í•œ ì„ì‹œ íŒŒì¼ ì‚­ì œ')
    }
    
    // ë³€í™˜ëœ íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œ
    if (req.file) {
      const wavPath = req.file.path.replace(/\.[^/.]+$/, '.wav')
      if (fs.existsSync(wavPath)) {
        fs.unlinkSync(wavPath)
        console.log('ë³€í™˜ëœ íŒŒì¼ ì‚­ì œë¨ (ì˜¤ë¥˜)')
      }
    }

    res.status(500).json({
      success: false,
      error: 'ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      details: error.message
    })
  }
})

// TTS (Text-to-Speech) ì—”ë“œí¬ì¸íŠ¸ (ì„ íƒì‚¬í•­)
app.post('/api/speak', async (req, res) => {
  try {
    const { text } = req.body

    if (!text) {
      return res.status(400).json({ error: 'í…ìŠ¤íŠ¸ê°€ í•„ìš”í•©ë‹ˆë‹¤.' })
    }

    const mp3 = await openai.audio.speech.create({
      model: 'tts-1',
      voice: 'alloy',
      input: text,
    })

    const buffer = Buffer.from(await mp3.arrayBuffer())
    
    res.set({
      'Content-Type': 'audio/mpeg',
      'Content-Length': buffer.length
    })
    
    res.send(buffer)

  } catch (error) {
    console.error('TTS error:', error)
    res.status(500).json({
      success: false,
      error: 'TTS ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      details: error.message
    })
  }
})

// GPT ì±—ë´‡ ì—”ë“œí¬ì¸íŠ¸
app.post('/api/chat', async (req, res) => {
  try {
    const { message, conversationHistory = [] } = req.body
    console.log('Chat request received:', message)
    if (!message) {
      return res.status(400).json({ error: 'ë©”ì‹œì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤.' })
    }

    const messages = [
      {
        role: 'system',
        content: 'ë‹¹ì‹ ì€ ê³¨í”„ì¥ ì˜ˆì•½ì„ ë„ì™€ì£¼ëŠ” ì¹œì ˆí•œ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì´í•´í•˜ê³  ì ì ˆí•œ ê³¨í”„ì¥ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.'
      },
      ...conversationHistory,
      {
        role: 'user',
        content: message
      }
    ]

    const completion = await openai.chat.completions.create({
      model: 'gpt-4',
      messages: messages,
      temperature: 0.7,
      max_tokens: 500
    })

    const reply = completion.choices[0].message.content

    res.json({
      success: true,
      reply: reply,
      usage: completion.usage
    })

  } catch (error) {
    console.error('Chat error:', error)
    res.status(500).json({
      success: false,
      error: 'ì±—ë´‡ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
      details: error.message
    })
  }
})

// ê±´ê°• ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
app.get('/api/health', (req, res) => {
  res.json({ status: 'ok', timestamp: new Date().toISOString() })
})

const PORT = process.env.PORT || 3001

app.listen(PORT, () => {
  console.log(`ğŸš€ Server is running on http://localhost:${PORT}`)
  console.log(`ğŸ“ API endpoints:`)
  console.log(`   - POST /api/transcribe - Audio to text`)
  console.log(`   - POST /api/speak - Text to speech`)
  console.log(`   - POST /api/chat - Chat with AI`)
  console.log(`   - GET  /api/health - Health check`)
})
